{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05 MNIST Softmax Classifier.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP1S7NCbWBt6OX1VhhoZAMt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"egGcPN7pitS3","colab_type":"code","colab":{}},"source":["!nvidia-smi\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","path = '/content/drive/My Drive'\n","os.chdir(path)\n","os.listdir(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ys1YQiAZv3dM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":689},"executionInfo":{"status":"ok","timestamp":1596178515921,"user_tz":-540,"elapsed":120860,"user":{"displayName":"xm zhao","photoUrl":"","userId":"05824450420151886703"}},"outputId":"5626b5fd-9a8c-4c7d-b0c9-d9a8799df83e"},"source":["import torch\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision import datasets\n","import torch.nn.functional as F\n","\n","\n","class Model(torch.nn.Module):\n","  def __init__(self):\n","    super(Model,self).__init__()\n","    self.l1 = torch.nn.Linear(784,512)\n","    self.l2 = torch.nn.Linear(512,256)\n","    self.l3 = torch.nn.Linear(256,128)\n","    self.l4 = torch.nn.Linear(128,64)\n","    self.l5 = torch.nn.Linear(64,10)\n","  def forward(self, x):\n","    '''\n","    输入的数据行状为(n张图片*1*28*28)\n","    为让模型处理数据，将输入reshape为(n张图片*784)\n","    784=1*28*28\n","    -1则表示，reshape时自动计算第二个维度大小为784时，第一个维度的大小\n","    即，自动计算，已知特征数为784时，样本的数量\n","    '''\n","    x = x.view(-1,784)\n","    x = F.relu(self.l1(x))\n","    x = F.relu(self.l2(x))\n","    x = F.relu(self.l3(x))\n","    x = F.relu(self.l4(x))\n","    return self.l5(x) #使用CrossEntropyLoss时，神经网络最后一层不需要加激活函数\n","\n","batch_size = 64\n","#Compose组合[]内的操作，顺序执行\n","transform = transforms.Compose([\n","  transforms.ToTensor(),          #把PIL Image(像素/维度28*28，每个像素取值0,1,2...,255)，转换成tensor(1*28*28 即channel*width*height，取值0~1)\n","  transforms.Normalize((0.1307,),(0.3081,)) #数据标准化，Normalize((均值,),(标准差,))                 \n","])\n","\n","train_set = datasets.MNIST(root='Colab Notebooks/dataset',#获取MNIST数据集并储存在本地指定位置\n","              train=True,           # train : True = 训练集\n","              transform=transform,       #把数据转换成tensor\n","              download=True)\n","test_set = datasets.MNIST(root='Colab Notebooks/dataset',\n","              train=False,          # train : False = 测试集\n","              transform=transform,\n","              download=True)\n","\n","#加载数据集\n","train_loader = DataLoader(dataset=train_set,\n","              batch_size=batch_size,\n","              shuffle=True)\n","test_loader = DataLoader(dataset=test_set,\n","              batch_size=batch_size,\n","              shuffle=False)#测试集用于测试，无必要做shuffle\n","\n","model = Model()\n","criterion = torch.nn.CrossEntropyLoss()\n","#使用CrossEntropyLoss时，神经网络最后一层不需要加激活函数。且target标签需要为LongTensor\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n","\n","\n","def train(epoch):\n","  running_loss = 0.0\n","  for batch_idx,(input,target) in enumerate(train_loader):\n","    optimizer.zero_grad()\n","    output = model(input)\n","    loss = criterion(output,target)\n","    loss.backward()\n","    optimizer.step()\n","\n","    running_loss += loss.item()\n","    if batch_idx % 300 == 299:#在每次的epoch中，每过300条数据输出一次\n","      print('[%d,%5d] loss: %.3f' % (epoch + 1, batch_idx +1, running_loss/300))\n","      running_loss = 0.0\n","\n","def test():\n","  correct = 0\n","  total = 0\n","  with torch.no_grad(): #torch.no_grad()不计算梯度\n","    for input,target in test_loader:\n","      output = model(input)\n","      _,predicted = torch.max(output.data,dim=1)#求10个里最大值的小标，对比第2个维度里找最大值。行是第一个维度，列是第二个维度。取一行里最大列的下标\n","      total+=target.size(0)#求所有数据的条数\n","      correct += (predicted == target).sum().item()\n","  print('Accuracy on test set:%d %%' % (100*correct/total))\n","\n","\n","for epoch in range(10):\n","  train(epoch)\n","  test()\n","\n","\n"],"execution_count":72,"outputs":[{"output_type":"stream","text":["[1,  300] loss: 2.181\n","[1,  600] loss: 0.827\n","[1,  900] loss: 0.430\n","Accuracy on test set:89 %\n","[2,  300] loss: 0.328\n","[2,  600] loss: 0.280\n","[2,  900] loss: 0.231\n","Accuracy on test set:94 %\n","[3,  300] loss: 0.182\n","[3,  600] loss: 0.177\n","[3,  900] loss: 0.155\n","Accuracy on test set:95 %\n","[4,  300] loss: 0.123\n","[4,  600] loss: 0.137\n","[4,  900] loss: 0.107\n","Accuracy on test set:96 %\n","[5,  300] loss: 0.098\n","[5,  600] loss: 0.098\n","[5,  900] loss: 0.086\n","Accuracy on test set:96 %\n","[6,  300] loss: 0.076\n","[6,  600] loss: 0.076\n","[6,  900] loss: 0.072\n","Accuracy on test set:97 %\n","[7,  300] loss: 0.061\n","[7,  600] loss: 0.062\n","[7,  900] loss: 0.060\n","Accuracy on test set:97 %\n","[8,  300] loss: 0.050\n","[8,  600] loss: 0.050\n","[8,  900] loss: 0.051\n","Accuracy on test set:97 %\n","[9,  300] loss: 0.037\n","[9,  600] loss: 0.041\n","[9,  900] loss: 0.046\n","Accuracy on test set:97 %\n","[10,  300] loss: 0.030\n","[10,  600] loss: 0.032\n","[10,  900] loss: 0.037\n","Accuracy on test set:97 %\n"],"name":"stdout"}]}]}
